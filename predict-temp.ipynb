{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
    "                              out_channels=4 * hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        \n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        \n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "        \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "        # Nueva capa de convolución para reducir a un solo canal de salida\n",
    "        self.conv_out = nn.Conv2d(hidden_dim[-1], 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "        \n",
    "        cur_layer_input = input_tensor\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(input_tensor.size(1)):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "            \n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "        # Tomamos la salida de la última capa y del último paso temporal\n",
    "        last_output = cur_layer_input[:, -1, :, :, :]  # (batch, hidden_dim[-1], H, W)\n",
    "        \n",
    "        # Aplicamos la convolución para reducir la salida a un solo canal\n",
    "        output = self.conv_out(last_output)  # (batch, 1, H, W)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "    \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================\n",
    "# Cargar y Organizar los Datos\n",
    "# ===================\n",
    "\n",
    "# Ruta donde están los archivos\n",
    "data_dir = 'data/tmean_interp_final/npy'\n",
    "\n",
    "# Obtener la lista de archivos .npy en orden temporal\n",
    "file_list = sorted(glob(os.path.join(data_dir, 'temperatura_*.npy')))\n",
    "\n",
    "# Cargar todos los archivos y convertirlos en un solo tensor\n",
    "data_list = [np.load(file) for file in file_list]  # Cada archivo debe ser de tamaño (271, 221)\n",
    "data = np.stack(data_list)  # Forma resultante: (T, H, W), donde T es el total de meses\n",
    "data = torch.tensor(data, dtype=torch.float32)  # Convertir a tensor de PyTorch\n",
    "\n",
    "# ===================\n",
    "# Preparar los Datos para el Dataset\n",
    "# ===================\n",
    "\n",
    "class TemperatureDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx:idx+self.seq_length], self.data[idx+self.seq_length])\n",
    "\n",
    "# Parámetros del modelo y de entrenamiento\n",
    "seq_length = 12  # Usaremos 12 meses para predecir el siguiente\n",
    "train_size = int(0.8 * len(data))  # División 80% para entrenamiento\n",
    "train_data, test_data = data[:train_size], data[train_size:]\n",
    "\n",
    "train_dataset = TemperatureDataset(train_data, seq_length)\n",
    "test_dataset = TemperatureDataset(test_data, seq_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 510.23734993770205\n",
      "Epoch 2/10, Loss: 442.2306381751751\n",
      "Epoch 3/10, Loss: 403.4614142056169\n",
      "Epoch 4/10, Loss: 367.67569390658673\n",
      "Epoch 5/10, Loss: 336.46757980872843\n",
      "Epoch 6/10, Loss: 306.74431952114764\n",
      "Epoch 7/10, Loss: 279.9421060496363\n",
      "Epoch 8/10, Loss: 255.22578377559267\n",
      "Epoch 9/10, Loss: 232.2980825489965\n",
      "Epoch 10/10, Loss: 211.39640071474273\n"
     ]
    }
   ],
   "source": [
    "# ===================\n",
    "# Configuración del modelo\n",
    "# ===================\n",
    "\n",
    "input_dim = 1  # Un canal de temperatura\n",
    "hidden_dim = [32, 32]\n",
    "kernel_size = (3,3)\n",
    "num_layers = 2\n",
    "\n",
    "model = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers, batch_first=True).to(\"cuda\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ===================\n",
    "# Entrenamiento del Modelo\n",
    "# ===================\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq, target in train_loader:\n",
    "        seq = seq.unsqueeze(2).to(\"cuda\")  # (batch, seq_length, 1, 271, 221)\n",
    "        target = target.unsqueeze(1).to(\"cuda\")  # (batch, 1, 271, 221)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Predicción\n",
    "# ===================\n",
    "\n",
    "def predict_future(data, model, steps):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    input_seq = data[-seq_length:]  # Usa los últimos `seq_length` pasos de tiempo, con forma (seq_length, 271, 221)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Añadimos las dimensiones de batch y canal\n",
    "        input_seq_tensor = input_seq.unsqueeze(0).unsqueeze(2)  # (1, seq_length, 1, 271, 221)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq_tensor)  # Salida de forma (1, 1, 271, 221)\n",
    "            next_pred = output.squeeze(0).squeeze(0)  # Redimensionamos a (271, 221)\n",
    "        \n",
    "        predictions.append(next_pred)\n",
    "        \n",
    "        # Ajustamos next_pred para que tenga la misma dimensión que input_seq[1:]\n",
    "        next_pred = next_pred.unsqueeze(0)  # (1, 271, 221)\n",
    "        input_seq = torch.cat((input_seq[1:], next_pred), dim=0)  # Concatenar en la dimensión de tiempo (0)\n",
    "\n",
    "    return torch.stack(predictions)\n",
    "\n",
    "\n",
    "# Predicción para los siguientes 6 meses\n",
    "num_future_months = 6\n",
    "future_predictions = predict_future(test_data, model, num_future_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la región para limitar el gráfico\n",
    "region = (-74.6, -72.4, 5.5, 8.2)\n",
    "\n",
    "# Cargar el shapefile de los límites administrativos de Colombia\n",
    "colombia_shapefile = 'data/aoi/Departamento.shp'\n",
    "gdf = gpd.read_file(colombia_shapefile)\n",
    "\n",
    "# Reproyectar a WGS84 si es necesario\n",
    "if gdf.crs != \"EPSG:4326\":\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Filtrar para el departamento de Santander\n",
    "santander_gdf = gdf[gdf['DeNombre'] == 'Santander']\n",
    "\n",
    "# Crear las coordenadas de latitud y longitud para la grilla\n",
    "latitudes = np.linspace(region[2], region[3], 271)\n",
    "longitudes = np.linspace(region[0], region[1], 221)\n",
    "lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n",
    "\n",
    "# Iterar sobre cada mes predicho en `future_predictions`\n",
    "for i, pred in enumerate(future_predictions):\n",
    "    # Convertir la predicción del mes actual a numpy\n",
    "    temperature_map = pred.numpy()  # Tiene forma (271, 221)\n",
    "\n",
    "    # Graficar la predicción de temperatura\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pcolormesh(lon_grid, lat_grid, temperature_map, cmap='coolwarm', shading='auto')\n",
    "    plt.colorbar(label='Temperatura (°C)')\n",
    "\n",
    "    # Superponer los límites administrativos de Santander\n",
    "    santander_gdf.boundary.plot(ax=plt.gca(), linewidth=1, edgecolor=\"black\", label=\"Límites de Santander\")\n",
    "\n",
    "    # Limitar los ejes al área de interés\n",
    "    plt.xlim(region[0], region[1])\n",
    "    plt.ylim(region[2], region[3])\n",
    "\n",
    "    # Etiquetas y título\n",
    "    plt.xlabel('Longitud')\n",
    "    plt.ylabel('Latitud')\n",
    "    plt.title(f'Mapa de temperatura predicho para el mes {i + 1}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEOHIDRO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
